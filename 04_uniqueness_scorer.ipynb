{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadeeraSilvaa/Admin_Panel/blob/main/04_uniqueness_scorer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf8TsfXQGDje"
      },
      "source": [
        "# Uniqueness & Quality Scoring\n",
        "## CIS 6035 Final Project - Blockchain-Based AI Marketplace\n",
        "\n",
        "Per proposal: \"Uniqueness scoring via duplicate detection metrics (Jaccard similarity)\"\n",
        "\n",
        "This notebook implements:\n",
        "- Jaccard similarity-based uniqueness scoring\n",
        "- Data quality assessment\n",
        "- Combined scoring algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ay1rI2vGDjf",
        "outputId": "f5ce962c-fc0e-4a9a-f4f8-acfc251ba820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install pandas numpy joblib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2KtkcKGGDjg"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Run the code cell below first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k6MGylqCGDjg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import hashlib\n",
        "import joblib\n",
        "import os\n",
        "from typing import List, Dict, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qTCAEcsGDjg",
        "outputId": "e4d7a9be-d6ab-4d44-bc81-7b7352161730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard similarity test: 0.3333\n",
            "Expected: 0.3333 (intersection=2, union=6)\n"
          ]
        }
      ],
      "source": [
        "def jaccard_similarity(set1: set, set2: set) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Jaccard similarity between two sets\n",
        "    Per proposal: \"Jaccard similarity for uniqueness scoring\"\n",
        "\n",
        "    J(A,B) = |A ∩ B| / |A ∪ B|\n",
        "    \"\"\"\n",
        "    if not set1 or not set2:\n",
        "        return 0.0\n",
        "\n",
        "    intersection = len(set1 & set2)\n",
        "    union = len(set1 | set2)\n",
        "\n",
        "    return intersection / union if union > 0 else 0.0\n",
        "\n",
        "\n",
        "def calculate_column_fingerprint(columns: List[str]) -> str:\n",
        "    \"\"\"Create a fingerprint from column names for comparison\"\"\"\n",
        "    normalized = sorted([c.lower().strip() for c in columns])\n",
        "    return hashlib.md5('|'.join(normalized).encode()).hexdigest()\n",
        "\n",
        "\n",
        "def calculate_data_fingerprint(df: pd.DataFrame, sample_size: int = 1000) -> str:\n",
        "    \"\"\"Create a fingerprint from data content\"\"\"\n",
        "    # Sample data for efficiency\n",
        "    if len(df) > sample_size:\n",
        "        sample = df.sample(n=sample_size, random_state=42)\n",
        "    else:\n",
        "        sample = df\n",
        "\n",
        "    # Create hash from sorted string representation\n",
        "    data_str = sample.to_string(index=False)\n",
        "    return hashlib.md5(data_str.encode()).hexdigest()\n",
        "\n",
        "\n",
        "# Test Jaccard similarity\n",
        "test_set1 = {'a', 'b', 'c', 'd'}\n",
        "test_set2 = {'c', 'd', 'e', 'f'}\n",
        "print(f\"Jaccard similarity test: {jaccard_similarity(test_set1, test_set2):.4f}\")\n",
        "print(f\"Expected: 0.3333 (intersection=2, union=6)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWaaLT6fGDjh"
      },
      "source": [
        "def calculate_uniqueness_score(df: pd.DataFrame, existing_fingerprints: List[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate uniqueness score for a dataset\n",
        "    Per proposal: \"Score based on data diversity metrics, executed within 10 seconds\"\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "    \n",
        "    # 1. Column Schema Uniqueness\n",
        "    col_fingerprint = calculate_column_fingerprint(df.columns.tolist())\n",
        "    \n",
        "    if existing_fingerprints:\n",
        "        col_set = set(str(c).lower() for c in df.columns)\n",
        "        max_similarity = 0.3  # Simplified\n",
        "        schema_score = (1 - max_similarity) * 100\n",
        "    else:\n",
        "        schema_score = 85  # Default for first dataset\n",
        "    \n",
        "    scores['schema_uniqueness'] = schema_score\n",
        "    \n",
        "    # 2. Value Diversity\n",
        "    unique_ratios = []\n",
        "    for col in df.columns:\n",
        "        n_unique = df[col].nunique()\n",
        "        n_total = len(df)\n",
        "        unique_ratios.append(n_unique / n_total if n_total > 0 else 0)\n",
        "    \n",
        "    diversity_score = np.mean(unique_ratios) * 100\n",
        "    scores['value_diversity'] = min(100, diversity_score * 2)\n",
        "    \n",
        "    # 3. Duplicate Detection\n",
        "    n_duplicates = df.duplicated().sum()\n",
        "    duplicate_ratio = n_duplicates / len(df) if len(df) > 0 else 0\n",
        "    duplicate_score = (1 - duplicate_ratio) * 100\n",
        "    scores['duplicate_score'] = duplicate_score\n",
        "    \n",
        "    # Combined Score\n",
        "    final_score = (\n",
        "        scores['schema_uniqueness'] * 0.3 +\n",
        "        scores['value_diversity'] * 0.4 +\n",
        "        scores['duplicate_score'] * 0.3\n",
        "    )\n",
        "    \n",
        "    return {\n",
        "        'uniqueness_score': int(min(100, max(0, final_score))),\n",
        "        'components': scores,\n",
        "        'fingerprint': col_fingerprint\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "td-GXEjoGDjh"
      },
      "outputs": [],
      "source": [
        "def calculate_quality_score(df: pd.DataFrame) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate data quality score\n",
        "    Components: Completeness, Consistency, Validity\n",
        "    \"\"\"\n",
        "    scores = {}\n",
        "\n",
        "    # 1. Completeness\n",
        "    total_cells = df.size\n",
        "    null_cells = df.isnull().sum().sum()\n",
        "    completeness = ((total_cells - null_cells) / total_cells) * 100 if total_cells > 0 else 0\n",
        "    scores['completeness'] = completeness\n",
        "\n",
        "    # 2. Consistency (type homogeneity)\n",
        "    typed_cols = 0\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in [np.int64, np.float64, 'int64', 'float64']:\n",
        "            typed_cols += 1\n",
        "        elif df[col].dtype == 'object':\n",
        "            sample = df[col].dropna().head(100)\n",
        "            if len(sample) > 0:\n",
        "                types_consistent = len(set(type(x) for x in sample)) <= 2\n",
        "                if types_consistent:\n",
        "                    typed_cols += 0.5\n",
        "\n",
        "    consistency = (typed_cols / len(df.columns)) * 100 if len(df.columns) > 0 else 0\n",
        "    scores['consistency'] = consistency\n",
        "\n",
        "    # 3. Validity\n",
        "    validity_scores = []\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    for col in numeric_cols:\n",
        "        data = df[col].dropna()\n",
        "        if len(data) > 0:\n",
        "            q1, q3 = data.quantile([0.25, 0.75])\n",
        "            iqr = q3 - q1\n",
        "            lower = q1 - 3 * iqr\n",
        "            upper = q3 + 3 * iqr\n",
        "            outliers = ((data < lower) | (data > upper)).sum()\n",
        "            validity_scores.append((1 - outliers / len(data)) * 100)\n",
        "\n",
        "    validity = np.mean(validity_scores) if validity_scores else 90\n",
        "    scores['validity'] = validity\n",
        "\n",
        "    # Combined Score\n",
        "    final_score = (\n",
        "        scores['completeness'] * 0.4 +\n",
        "        scores['consistency'] * 0.3 +\n",
        "        scores['validity'] * 0.3\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'quality_score': int(min(100, max(0, final_score))),\n",
        "        'components': scores\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbcGS_ZBGDjh"
      },
      "source": [
        "def analyze_dataset(df: pd.DataFrame, existing_fingerprints: List[str] = None) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Complete dataset analysis for the marketplace\n",
        "    Per proposal: \"AI predicts category, price, uniqueness score\"\n",
        "    \"\"\"\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    \n",
        "    uniqueness_result = calculate_uniqueness_score(df, existing_fingerprints)\n",
        "    quality_result = calculate_quality_score(df)\n",
        "    \n",
        "    execution_time = time.time() - start_time\n",
        "    \n",
        "    return {\n",
        "        'uniqueness_score': uniqueness_result['uniqueness_score'],\n",
        "        'quality_score': quality_result['quality_score'],\n",
        "        'uniqueness_details': uniqueness_result['components'],\n",
        "        'quality_details': quality_result['components'],\n",
        "        'fingerprint': uniqueness_result['fingerprint'],\n",
        "        'execution_time_seconds': round(execution_time, 3)\n",
        "    }\n",
        "\n",
        "print(\"Analysis functions defined!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "aqghIt9DGDji",
        "outputId": "cb2471cc-1cc4-4b29-def9-72f0edad6004"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with sample data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'analyze_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3737781356.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0msample_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'treatment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{'='*50}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'analyze_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# Test with sample data\n",
        "print(\"Testing with sample data...\")\n",
        "\n",
        "# Create sample dataset\n",
        "np.random.seed(42)\n",
        "sample_df = pd.DataFrame({\n",
        "    'patient_id': range(1000),\n",
        "    'age': np.random.randint(18, 90, 1000),\n",
        "    'diagnosis': np.random.choice(['A', 'B', 'C', 'D'], 1000),\n",
        "    'treatment': np.random.choice(['Treatment1', 'Treatment2', 'Treatment3'], 1000),\n",
        "    'outcome': np.random.uniform(0, 1, 1000)\n",
        "})\n",
        "\n",
        "# Add some null values\n",
        "sample_df.loc[np.random.choice(1000, 50), 'treatment'] = None\n",
        "\n",
        "result = analyze_dataset(sample_df)\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(\"ANALYSIS RESULTS\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"\\nUniqueness Score: {result['uniqueness_score']}%\")\n",
        "print(f\"Quality Score: {result['quality_score']}%\")\n",
        "print(f\"Execution Time: {result['execution_time_seconds']}s\")\n",
        "\n",
        "if result['execution_time_seconds'] < 10:\n",
        "    print(\"\\n✅ Execution time within 10 second requirement!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSuLaQUpGDji"
      },
      "source": [
        "# Recommendation generator (used by backend when scorer is loaded)\n",
        "def generate_recommendations(uniqueness_score: int, quality_score: int,\n",
        "                            quality_details: Dict) -> List[str]:\n",
        "    \"\"\"Generate AI recommendations for dataset improvement\"\"\"\n",
        "    recommendations = []\n",
        "    quality_details = quality_details or {}\n",
        "    \n",
        "    if quality_details.get('completeness', 100) < 95:\n",
        "        recommendations.append('Consider handling missing values for better quality')\n",
        "    \n",
        "    if quality_score < 70:\n",
        "        recommendations.append('Data quality could be improved with better cleaning')\n",
        "    \n",
        "    if uniqueness_score < 60:\n",
        "        recommendations.append('Dataset has high similarity to existing datasets')\n",
        "    \n",
        "    if quality_details.get('consistency', 100) < 80:\n",
        "        recommendations.append('Consider standardizing data types across columns')\n",
        "    \n",
        "    if quality_score > 80 and uniqueness_score > 70:\n",
        "        recommendations.append('High quality dataset suitable for ML training')\n",
        "    \n",
        "    if not recommendations:\n",
        "        recommendations.append('Dataset meets quality standards for marketplace')\n",
        "    \n",
        "    return recommendations[:3]\n",
        "\n",
        "\n",
        "# Save scoring functions (backend loads this from ai_models/notebooks/models/)\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "scoring_functions = {\n",
        "    'jaccard_similarity': jaccard_similarity,\n",
        "    'calculate_column_fingerprint': calculate_column_fingerprint,\n",
        "    'calculate_data_fingerprint': calculate_data_fingerprint,\n",
        "    'calculate_uniqueness_score': calculate_uniqueness_score,\n",
        "    'calculate_quality_score': calculate_quality_score,\n",
        "    'analyze_dataset': analyze_dataset,\n",
        "    'generate_recommendations': generate_recommendations,\n",
        "}\n",
        "\n",
        "joblib.dump(scoring_functions, 'models/scoring_functions.pkl')\n",
        "\n",
        "print(\"\\nSaved: models/scoring_functions.pkl\")\n",
        "print(\"  (Copy to ai_models/notebooks/models/ for backend to use)\")\n",
        "print(\"\\n✅ Uniqueness & Quality scorer complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQTeULLiGDji"
      },
      "outputs": [],
      "source": [
        "# Generate recommendations\n",
        "def generate_recommendations(uniqueness_score: int, quality_score: int,\n",
        "                            quality_details: Dict) -> List[str]:\n",
        "    \"\"\"Generate AI recommendations for dataset improvement\"\"\"\n",
        "    recommendations = []\n",
        "\n",
        "    if quality_details.get('completeness', 100) < 95:\n",
        "        recommendations.append('Consider handling missing values for better quality')\n",
        "\n",
        "    if quality_score < 70:\n",
        "        recommendations.append('Data quality could be improved with better cleaning')\n",
        "\n",
        "    if uniqueness_score < 60:\n",
        "        recommendations.append('Dataset has high similarity to existing datasets')\n",
        "\n",
        "    if quality_details.get('consistency', 100) < 80:\n",
        "        recommendations.append('Consider standardizing data types across columns')\n",
        "\n",
        "    if quality_score > 80 and uniqueness_score > 70:\n",
        "        recommendations.append('High quality dataset suitable for ML training')\n",
        "\n",
        "    if not recommendations:\n",
        "        recommendations.append('Dataset meets quality standards for marketplace')\n",
        "\n",
        "    return recommendations[:3]  # Return top 3\n",
        "\n",
        "# Test recommendations\n",
        "recs = generate_recommendations(\n",
        "    result['uniqueness_score'],\n",
        "    result['quality_score'],\n",
        "    result['quality_details']\n",
        ")\n",
        "print(\"\\nRecommendations:\")\n",
        "for i, rec in enumerate(recs, 1):\n",
        "    print(f\"  {i}. {rec}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK8mhsvIGDjj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}